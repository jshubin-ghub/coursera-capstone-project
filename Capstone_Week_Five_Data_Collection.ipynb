{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Capstone Week Five Data Collection Notebook\n\nThis notebook contains the code for harvesting venue data from FourSquare and Storing it in a dictionary.  To re-use this code you may have to run the search routine muliple times on separate days to accomodate the query limit (quota) imposed by FourSquare."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Folium installed and imported!\n"
                }
            ],
            "source": "#!/Users/john/anaconda3/bin/conda install -c conda-forge folium=0.10.0 --yes\nimport folium\n\nprint('Folium installed and imported!')"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Your credentails:\n"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Retrieve Previously Defined Neighborhoods\nNeighborhoods are defined, selected, and stored in a separate notebook available on [Github](github.com/jshubin-ghub/coursera-capstone-project/blob/master/Capstone_Week_Five_Neighborhoods.ipynb)."
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\n\nnewyork_geo_df = pd.read_pickle(\"new_york_n_geodata.pkl\") \ntoronto_geo_df = pd.read_pickle(\"toronto_n_geodata.pkl\")\n\n# a dataframe for testing\nnewyork_geo_test_df = newyork_geo_df.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Getting Venues From FourSquare\n\nAfter some experimentation (documentation for FourSquare is sparse) I found using the FourSquare search endpoint, specifying the 'browse' intent produced the broadest results, with the slight drawback that some venues outside of the specified radius were returned.  "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# search_browse_intent_json_to_dataframe has a bug and \n# returns a dict of category values instead of just the category_id\ndef search_browse_intent_json_to_dataframe(current_result, geo_dataframe, index, current_df):\n    \"\"\"\n    Data from current_result json is parsed and added to current_df dataframe.\n\n    Parameters\n    ----------\n    current_result : json \n        json returned from foursquare search endpoint with intent = browse modifier \n    geo_dataframe : DataFrame \n        dataframe of data linked to geolocations\n    index : int\n        current record of interest in the geo_dataframe data\n    current_df : DataFrame\n        information collected from previously parsed foursquare queries\n\n    Returns\n    -------\n    DataFrame   \n    \"\"\"\n\n    try: \n        neighborhood_label = geo_dataframe.at[index, 'ZIPCode']\n    except:\n        neighborhood_label = geo_dataframe.at[index, 'PostalCode']\n    for item in range(len(current_result['response']['venues'])):\n        usable_row = False\n        \n        try:\n            new_row_list = [neighborhood_label,\n                           current_result['response']['venues'][item]['name'],\n                           current_result['response']['venues'][item]['id'],\n                           current_result['response']['venues'][item]['location']['lat'],\n                           current_result['response']['venues'][item]['location']['lng']]\n        except:\n            print(\"error creating new row list at index {} in result list {} items long\".format(item,len(current_result['response']['venues'])))\n        try:\n            if len(current_result['response']['venues'][item]['categories'][0])>0:\n                new_row_list.append(current_result['response']['venues'][item]['categories'][0])\n                distance_from_center = distance((geo_dataframe.at[index, 'Latitude'],\n                                                 geo_dataframe.at[index, 'Longitude']),\n                                                (new_row_list[3],new_row_list[4]))\n                    \n                if distance_from_center <1.0:\n                    usable_row = True\n                else:\n                    print(\"point too far away\", distance_from_center)\n            else:\n                print(\"no category found for:{}\".format(new_row_list))                \n        except IndexError as error:\n            print (error)\n            print (\"IndexError exception thrown for:{}\".format(new_row_list))\n\n        # add the venue if it isn't alread in the dataframe, based on venue_id\n        if usable_row:\n            new_row_df = pd.DataFrame([new_row_list], columns = ['ZIPCode', 'venue_name', 'venue_id', 'latitude', 'longitude', 'category_id'])\n            if new_row_df['venue_id'][0] not in current_df['venue_id']:\n                current_df = current_df.append(new_row_df, ignore_index = True)\n    # add the venue if it isn't alread in the dataframe, based on venue_id    \n    return (current_df)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# here are a couple of helper functions\n# distance computes distance between two geocoordinates (in kilometers)\n# spread_out computes a rosette of n points around a central geocoordinate.\nimport math\n\ndef distance(origin, destination):\n    lat1, lon1 = origin\n    lat2, lon2 = destination\n    radius = 6371 # radius of the earth in km, returned distance will be in kilometers\n\n    dlat = math.radians(lat2-lat1)\n    dlon = math.radians(lon2-lon1)\n    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n    d = radius * c\n\n    return d\n\n\ndef spread_out(latitude, longitude, num_points, distance):\n\n    \n    # create a rosette of points at a distance from a central point at latitude, longitude\n    # the number of points indicates the rosette petals arround the central point\n    # center point is returned with the rosette as the first value of a list\n    # distance is in meters, this function assumes spherical geometry for changes in latitude spacing.\n    radius = 6371 # radius of the earth in km\n    km_distance = distance/1000\n    \n    # number_degrees will be the number of latitude degrees of displacements for passed distance\n    number_degrees = 90*(km_distance/((1/4)* 2*math.pi*radius))\n    # longitude displacement will be divded by cos(latitude)\n    a = math.cos((abs(latitude)/360)*2*math.pi)\n    \n    rotation = 2*math.pi/num_points\n    \n    rosette = [(latitude, longitude)]\n    for spot in range(num_points):\n        spot_rotation = spot*rotation\n        spot_lat = latitude + (math.sin(spot_rotation)*number_degrees)\n        spot_lon = longitude + (math.cos(spot_rotation)*number_degrees)/a\n        rosette.append((spot_lat, spot_lon))\n    return rosette\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import pickle\n\ntry:\n    with open('ny_results.pkl', 'rb') as handle:\n        ny_results_dict = pickle.load(handle)\n        print('loading ny_results_dict from filesystem')\nexcept Exception as e: \n    print(\"Exception caught: {}\".format(e))\n    print(\"creating new ny_results_dict\")\nny_index_series = newyork_geo_df.index\n# for index in ny_index_series:\n#     if (newyork_geo_df.at(index,'ZIPCode')) in ny_results_dict.keys:\n#         print('found {} in results dict'.format(newyork_geo_df[index]))\nfor index in ny_index_series:\n    if newyork_geo_df.at[index,'ZIPCode'] in ny_results_dict:\n        print('skipping: {}'.format(newyork_geo_df.at[index,'ZIPCode']))\n    else:\n        print(\"new entry: {}\".format(newyork_geo_df.at[index,'ZIPCode']))\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## New York Queries"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Harvest New York foursquare venues at a geocoordinate, using the search endpoint with 'browse' intent\n\n# Duplicate venues, venues outside geographical limits, and records with incomplete \n#  category information are removed.\n# \n# Because the number of searches required exceeded the quota imposed by the Foursquare, intermediate \n#  results are stored and retrieved as query quota is refreshed.\n#\n# Results are a dict of dataframes with ZIPCode keys.\n\nimport json\nimport requests\nimport pandas as pd\nimport time\n\ntry:\n    with open('ny_results.pkl', 'rb') as handle:\n        ny_results_dict = pickle.load(handle)\n        print('loading ny_results_dict from filesystem')\nexcept:\n    print(\"creating new ny_results_dict\")\n    ny_results_dict = {} # will be used to gather \n###(Changed for Testing)\nny_index_series = newyork_geo_df.index \n#ny_index_series = newyork_geo_test_df.index\n\nfor index in ny_index_series:\n    print('*****************************')\n    print('***** Running queries for zip code: {}  *****'.format(newyork_geo_df.at[index, 'ZIPCode']))\n    print('*****************************')\n    \n    # set up a dataframe to gather unique venue information from multiple searches\n    if newyork_geo_df.at[index,'ZIPCode'] in ny_results_dict:\n        print('skipping: {}'.format(newyork_geo_df.at[index,'ZIPCode']))\n    else:\n        search_results_df = pd.DataFrame(columns = ['ZIPCode', \n                                                    'venue_name', \n                                                    'venue_id', \n                                                    'latitude', \n                                                    'longitude', \n                                                    'category_id'])\n\n\n        ###(Changed for testing)\n        lat, lng = newyork_geo_df.at[index, 'Latitude'], newyork_geo_df.at[index, 'Longitude'] \n        #lat, lng = newyork_geo_test_df.at[index, 'Latitude'], newyork_geo_test_df.at[index, 'Longitude']\n\n        rosette = spread_out(lat, lng, 6, 500)\n        for spot in rosette:\n            time.sleep(2)\n            lat, lng = spot[0], spot[1]\n            radius = 500\n            LIMIT = 1000\n\n            url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&intent=browse'.format(\n                    CLIENT_ID, \n                    CLIENT_SECRET, \n                    VERSION, \n                    lat, \n                    lng, \n                    radius, \n                    LIMIT)\n            print('*****************************')\n            print('***** Running query at lat:{}, lon:{} *****'.format(lat,lng))\n            print('*****************************')\n            this_result = requests.get(url).json()\n            \n            search_results_df = search_browse_intent_json_to_dataframe(this_result, \n                                                                       newyork_geo_df,\n                                                                       index,\n                                                                       search_results_df)\n\n        ny_results_dict[newyork_geo_df.at[index, 'ZIPCode']] = search_results_df\n\nprint(\"successfully completed data harvest for new york\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# save the collected data\nimport pickle\n\nwith open('ny_results.pkl', 'wb') as handle:\n    pickle.dump(ny_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Toronto Queries"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Harvest toronto foursquare venues at a geocoordinate, using the search endpoint with 'browse' intent\n# Code is from 'Harvest New York foursquare venues', above\n\nimport json\nimport requests\nimport pandas as pd\nimport time\n\n\ntry:\n    with open('toronto_results.pkl', 'rb') as handle:\n        toronto_results_dict = pickle.load(handle)\n        print('loading toronto_results_dict from filesystem')\nexcept:\n    print(\"creating new toronto_results_dict\")\n    toronto_results_dict = {} # will be used to gather venue dataframes for each PostalCode\n###(Changed for Testing)\ntoronto_index_series = toronto_geo_df.index \n\n\nfor index in toronto_index_series:\n    print('*****************************')\n    print('***** Running queries for postal code: {}  *****'.format(toronto_geo_df.at[index, 'PostalCode']))\n    print('*****************************')\n    \n    # set up a dataframe to gather unique venue information from multiple searches\n    if toronto_geo_df.at[index,'PostalCode'] in toronto_results_dict:\n        print('skipping: {}'.format(toronto_geo_df.at[index,'PostalCode']))\n    else:\n        search_results_df = pd.DataFrame(columns = ['PostalCode', \n                                                    'venue_name', \n                                                    'venue_id', \n                                                    'latitude', \n                                                    'longitude', \n                                                    'category_id'])\n\n        lat, lng = toronto_geo_df.at[index, 'Latitude'], toronto_geo_df.at[index, 'Longitude'] \n\n        rosette = spread_out(lat, lng, 6, 500)\n        for spot in rosette:\n            time.sleep(2)\n            lat, lng = spot[0], spot[1]\n            radius = 500\n            LIMIT = 1000\n   \n            url = 'https://api.foursquare.com/v2/venues/search?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&intent=browse'.format(\n                    CLIENT_ID, \n                    CLIENT_SECRET, \n                    VERSION, \n                    lat, \n                    lng, \n                    radius, \n                    LIMIT)\n            print('*****************************')\n            print('***** Running query at lat:{}, lon:{} *****'.format(lat,lng))\n            print('*****************************')\n            this_result = requests.get(url).json()\n            \n            search_results_df = search_browse_intent_json_to_dataframe(this_result, \n                                                                       toronto_geo_df,\n                                                                       index,\n                                                                       search_results_df)\n\n        \n\n\n        # displaced searches completed and unique results have been collected\n\n\n        toronto_results_dict[toronto_geo_df.at[index, 'PostalCode']] = search_results_df\n\nprint(\"successfully completed data harvest for toronto\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# save the collected data\nimport pickle\n\nwith open('toronto_results.pkl', 'wb') as handle:\n    pickle.dump(toronto_results_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "loading ny_results_dict from filesystem\nloading toronto_results_dict from filesystem\n"
                }
            ],
            "source": "import pickle\nimport pandas as pd\n# get the ny data back\ntry:\n    with open('ny_results.pkl', 'rb') as handle:\n        ny_results_dict = pickle.load(handle)\n        print('loading ny_results_dict from filesystem')\nexcept Exception as e: \n    print(\"Exception caught: {}\".format(e))\n# repeat data munge for toronto neighborhoods\ntry:\n    with open('toronto_results.pkl', 'rb') as handle:\n        toronto_results_dict = pickle.load(handle)\n        print('loading toronto_results_dict from filesystem')\nexcept Exception as e: \n    print(\"Exception caught: {}\".format(e))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Repair\n\nData in the 'category_id' field needs to be repaired"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "# repair New York neighborhood venues, splitting data in the 'category_id' category from a dict of values \n# mistakenly included in column 'category_id'\n\nny_cleaned_results = {}\nfor postcode in ny_results_dict:\n        first = False\n        this_df = pd.DataFrame(ny_results_dict[postcode])\n        \n        for index in this_df.index:\n            this_df.at[index,'new_category']=this_df.at[index,'category_id']['id']\n            \n        new_df = this_df.drop(columns =['category_id'])\n        ny_cleaned_results[postcode]=new_df\n        "
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# repair Toronto neighborhood venues, extracting the category_id from the dict of values \n# mistakenly included in column 'category_id', and some mislabeled columns\n\ntoronto_cleaned_results = {}\nfor postcode in toronto_results_dict:\n    this_df = pd.DataFrame(toronto_results_dict[postcode])\n    for index in this_df.index:\n        this_df.at[index,'new_category']=this_df.at[index,'category_id']['id']    \n    new_df = this_df.drop(columns =['category_id', 'PostalCode'])\n    new_df.rename(columns = {'ZIPCode':'PostalCode'}, inplace = True)\n    toronto_cleaned_results[postcode]=new_df"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# save the corrected data\n\nwith open('ny_cleaned_results.pkl', 'wb') as handle:\n    pickle.dump(ny_cleaned_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('toronto_cleaned_results.pkl', 'wb') as handle:\n    pickle.dump(toronto_cleaned_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}